{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# README\n",
        "\n",
        "## Directory structure for running\n",
        "```\n",
        "root/\n",
        "├── training_data/\n",
        "│   └── ED/\n",
        "│       ├── train.csv      \n",
        "│       └── dev.csv       \n",
        "├── test_data/\n",
        "│   └── ED/\n",
        "│       └── test.csv       # Test data for demo\n",
        "├── bert_cnn_multi.ipynb  # Notebook\n",
        "├── bert_cnn_model_256_multi/      # Directory for the trained model\n",
        "└── bert_cnn_tokenizer_256_multi/  # Directory for the tokenizer\n",
        "\n",
        "```\n",
        "\n",
        "### Model can be downloaded from [here](https://drive.google.com/drive/folders/1-_eka-0MsXlYmHGJDICqZJBYtkpvPGYF?usp=sharing)\n",
        "### Tokenizer can be downloaded from [here](https://drive.google.com/drive/folders/1-UzHyEx1RSmysrhRTIt5DmnDu7BzbMhZ?usp=sharing)\n",
        "\n",
        "## Misc\n",
        "\n",
        "- The notebook is structured in 3 sections seperated my markdown cells for Training, Evaluation, and Demo\n",
        "- Model was inspired by the following [paper](https://www.sciencedirect.com/science/article/pii/S187705092300234X)\n"
      ],
      "metadata": {
        "id": "McaKBTVnhVfS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tf8K52YUMzV"
      },
      "outputs": [],
      "source": [
        "# Uncomment if using drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# import os\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "# os.chdir('/content/drive/My Drive/NLU')\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow transformers optuna"
      ],
      "metadata": {
        "id": "jupr_jKqVilc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "6l9TAs1nUYvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, TFBertModel\n",
        "import optuna"
      ],
      "metadata": {
        "id": "gtFv3nj6bvEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_FILE = \"./training_data/ED/train.csv\"\n",
        "VALIDATION_FILE = \"./training_data/ED/dev.csv\"\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "9meG_pvWZHCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(TRAINING_FILE)\n",
        "val_df = pd.read_csv(VALIDATION_FILE)"
      ],
      "metadata": {
        "id": "CgBSEO3fdvgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load bert tokenizer and tokenize our claims and evidence\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "train_encodings = tokenizer(\n",
        "    train_df[\"Claim\"].tolist(),\n",
        "    train_df[\"Evidence\"].tolist(),\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=256\n",
        ")\n",
        "\n",
        "val_encodings = tokenizer(\n",
        "    val_df[\"Claim\"].tolist(),\n",
        "    val_df[\"Evidence\"].tolist(),\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=256\n",
        ")\n"
      ],
      "metadata": {
        "id": "YvAoG7Ygb92m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert our data to a tensorflow dataset\n",
        "train_labels = train_df[\"label\"]\n",
        "val_labels = val_df[\"label\"]\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))\n"
      ],
      "metadata": {
        "id": "WJ1hINCggsA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get class weights\n",
        "train_class_counts = train_df[\"label\"].value_counts().to_dict()\n",
        "total_samples = len(train_df)\n",
        "class_weights = {\n",
        "    label: total_samples / (len(train_class_counts) * count)\n",
        "    for label, count in train_class_counts.items()\n",
        "}\n",
        "print(\"\\nClass weights:\", class_weights)\n",
        "print(train_class_counts)\n",
        "\n",
        "class_weight_dict = {int(cls): weight for cls, weight in class_weights.items()}"
      ],
      "metadata": {
        "id": "U2zNAsgNU5qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set batch size and shuffle data\n",
        "batch_size = 16\n",
        "\n",
        "train_dataset = train_dataset.shuffle(1000).batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n"
      ],
      "metadata": {
        "id": "7rV2dTaxg6m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BertCNNModel(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 num_filters=128,\n",
        "                 kernel_sizes=[3,4,5],\n",
        "                 dropout_rate=0.2,\n",
        "                 dense_units=128):\n",
        "        super(BertCNNModel, self).__init__()\n",
        "        self.bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.convs = [\n",
        "            tf.keras.layers.Conv1D(filters=num_filters,\n",
        "                                   kernel_size=k,\n",
        "                                   activation='relu',\n",
        "                                   padding='valid')\n",
        "            for k in kernel_sizes\n",
        "        ]\n",
        "        self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dense = tf.keras.layers.Dense(dense_units, activation='relu')\n",
        "        self.classifier = tf.keras.layers.Dense(2)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        bert_output = self.bert(inputs, training=training).last_hidden_state\n",
        "\n",
        "        # apply convolutions and pooling for each filter size\n",
        "        conv_outputs = []\n",
        "        for conv in self.convs:\n",
        "            x = conv(bert_output)\n",
        "            x = self.pool(x)\n",
        "            conv_outputs.append(x)\n",
        "\n",
        "        concated_conv_outputs = tf.concat(conv_outputs, axis=-1)\n",
        "        dropout_output = self.dropout(concated_conv_outputs, training=training)\n",
        "        dense_output = self.dense(dropout_output)\n",
        "        logits = self.classifier(dense_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "RD3Yxp35f5Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter fine-tuning\n",
        "def objective(trial):\n",
        "    num_filters = trial.suggest_categorical('num_filters', [64, 128, 256])\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.3, step=0.1)\n",
        "    dense_units = trial.suggest_categorical('dense_units', [64, 128, 256])\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 2e-5)\n",
        "\n",
        "\n",
        "    model = BertCNNModel(num_filters=num_filters,\n",
        "                                  dropout_rate=dropout_rate,\n",
        "                                  dense_units=dense_units, kernel_size=3)\n",
        "\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(train_dataset,\n",
        "                        validation_data=val_dataset,\n",
        "                        epochs=2,\n",
        "                        verbose=0,\n",
        "                        class_weight=class_weight_dict\n",
        "                        )\n",
        "\n",
        "    # return the highest accuracy over all epochs\n",
        "    best_val_accuracy = max(history.history['val_accuracy'])\n",
        "    return best_val_accuracy\n"
      ],
      "metadata": {
        "id": "o5kG1KLPwrCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the trial then print the results\n",
        "\n",
        "# study = optuna.create_study(direction=\"maximize\")\n",
        "# study.optimize(objective, timeout=7200, show_progress_bar=True)\n",
        "\n",
        "# print(\"Best trial:\")\n",
        "# trial = study.best_trial\n",
        "# print(trial.params)\n",
        "\n",
        "# print(\"============================TRAIL DATA=========================================\")\n",
        "# trials = study.get_trials()\n",
        "# for t in trials:\n",
        "#   print(t.params, t.value)"
      ],
      "metadata": {
        "id": "TMu2JyhX5dQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertCNNModel(num_filters=256, dropout_rate=0.2, dense_units=256)"
      ],
      "metadata": {
        "id": "0nU5uEsxhuFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "osBL7CRrhxp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# callback to save our model at best epoch\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"./bert_cnn_model_256_multi\",\n",
        "    save_best_only=True,\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"max\"\n",
        ")\n",
        "\n",
        "# train our model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=2,\n",
        "    verbose=1,\n",
        "    class_weight=class_weight_dict\n",
        "\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "-0FTzoQKiotx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"./bert_cnn_model_256_multi\")\n",
        "\n",
        "tokenizer.save_pretrained(\"./bert_cnn_tokenizer_256_multi\")"
      ],
      "metadata": {
        "id": "nrwAl-O-ixuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "bEfMdlVGUfpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel, TFBertForSequenceClassification\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "EmF45SiN8KjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_FILE = \"./training_data/ED/dev.csv\"\n",
        "test_df = pd.read_csv(TEST_FILE)\n"
      ],
      "metadata": {
        "id": "-g7uT78r8W0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"./bert_cnn_tokenizer_256_multi\")\n",
        "\n",
        "# Tokenize our claims and evidence\n",
        "test_encodings = tokenizer(\n",
        "    test_df[\"Claim\"].tolist(),\n",
        "    test_df[\"Evidence\"].tolist(),\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=256\n",
        ")"
      ],
      "metadata": {
        "id": "vJdNhWlS8-8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert tokenized encodings to a tensorflow dataset\n",
        "test_labels = test_df[\"label\"]\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))\n"
      ],
      "metadata": {
        "id": "4sJgEt5Z-2zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy of the model class used in training, for the model to use during inference\n",
        "class BertCNNModel(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 bert_model_name='bert-base-uncased',\n",
        "                 num_filters=128,\n",
        "                 kernel_sizes=[3,4,5],\n",
        "                 dropout_rate=0.2,\n",
        "                 dense_units=128):\n",
        "        super(BertCNNModel, self).__init__()\n",
        "        self.bert = TFBertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "        self.convs = [\n",
        "            tf.keras.layers.Conv1D(filters=num_filters,\n",
        "                                   kernel_size=k,\n",
        "                                   activation='relu',\n",
        "                                   padding='valid')\n",
        "            for k in kernel_sizes\n",
        "        ]\n",
        "        self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dense = tf.keras.layers.Dense(dense_units, activation='relu')\n",
        "        self.classifier = tf.keras.layers.Dense(2)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        bert_output = self.bert(inputs, training=training).last_hidden_state\n",
        "\n",
        "        conv_outputs = []\n",
        "        for conv in self.convs:\n",
        "            x = conv(bert_output)\n",
        "            x = self.pool(x)\n",
        "            conv_outputs.append(x)\n",
        "\n",
        "        concat = tf.concat(conv_outputs, axis=-1)\n",
        "        dropout_output = self.dropout(concat, training=training)\n",
        "        dense_output = self.dense(dropout_output)\n",
        "        logits = self.classifier(dense_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ELfEEd0YGNRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = tf.keras.models.load_model(\"./bert_cnn_model_256_multi\")"
      ],
      "metadata": {
        "id": "cL9UnetV_Rlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch the dataset so into the correct shape for the model\n",
        "test_dataset_batched = test_dataset.batch(16)"
      ],
      "metadata": {
        "id": "1IvJOg_FL2Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert out logits output to a class prediction\n",
        "predictions_logits = model.predict(test_dataset_batched)\n",
        "y_pred = np.argmax(predictions_logits, axis=1)"
      ],
      "metadata": {
        "id": "SdVu801w_NHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get accuracy and weighted/macro precision, recall, F1\n",
        "y_true = np.concatenate([y for _, y in test_dataset_batched], axis=0)\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", acc, \"\\n\")\n",
        "\n",
        "\n",
        "print(\"Macro\")\n",
        "prec = precision_score(y_true, y_pred, average=\"macro\")\n",
        "rec = recall_score(y_true, y_pred, average=\"macro\")\n",
        "f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "print(\"Precision:\", prec)\n",
        "print(\"Recall:\", rec)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "print(\"Weighted: \")\n",
        "prec = precision_score(y_true, y_pred, average='weighted')\n",
        "rec = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(\"Precision:\", prec)\n",
        "print(\"Recall:\", rec)\n",
        "print(\"F1 Score:\", f1, \"\\n\")\n"
      ],
      "metadata": {
        "id": "Vng4cG0yhu9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "wk0HZT5QVHQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel, TFBertForSequenceClassification\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "UZQ8ruYoV25E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_FILE = \"./test_data/ED/test.csv\"\n",
        "test_df = pd.read_csv(TEST_FILE)"
      ],
      "metadata": {
        "id": "LX6KU2H3V25K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"./bert_cnn_tokenizer_256_multi\")\n",
        "\n",
        "# Tokenize our claims and evidence\n",
        "test_encodings = tokenizer(\n",
        "    test_df[\"Claim\"].tolist(),\n",
        "    test_df[\"Evidence\"].tolist(),\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=256\n",
        ")"
      ],
      "metadata": {
        "id": "yUYhq_XrV25K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert tokenized encodings to a tensorflow dataset\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings)\n",
        "))"
      ],
      "metadata": {
        "id": "WV53NKsahk2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy of the model class used in training, for the model to use during inference\n",
        "class BertCNNModel(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 bert_model_name='bert-base-uncased',\n",
        "                 num_filters=128,\n",
        "                 kernel_sizes=[3,4,5],\n",
        "                 dropout_rate=0.2,\n",
        "                 dense_units=128):\n",
        "        super(BertCNNModel, self).__init__()\n",
        "        self.bert = TFBertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "        self.convs = [\n",
        "            tf.keras.layers.Conv1D(filters=num_filters,\n",
        "                                   kernel_size=k,\n",
        "                                   activation='relu',\n",
        "                                   padding='valid')\n",
        "            for k in kernel_sizes\n",
        "        ]\n",
        "        self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dense = tf.keras.layers.Dense(dense_units, activation='relu')\n",
        "        self.classifier = tf.keras.layers.Dense(2)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        bert_output = self.bert(inputs, training=training).last_hidden_state\n",
        "\n",
        "        conv_outputs = []\n",
        "        for conv in self.convs:\n",
        "            x = conv(bert_output)\n",
        "            x = self.pool(x)\n",
        "            conv_outputs.append(x)\n",
        "\n",
        "        concat = tf.concat(conv_outputs, axis=-1)\n",
        "        dropout_output = self.dropout(concat, training=training)\n",
        "        dense_output = self.dense(dropout_output)\n",
        "        logits = self.classifier(dense_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "IYrD3bx8V25K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = tf.keras.models.load_model(\"./bert_cnn_model_256_multi\")"
      ],
      "metadata": {
        "id": "7zm63xXaV25L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch the dataset so into the correct shape for the model\n",
        "test_dataset_batched = test_dataset.batch(16)\n"
      ],
      "metadata": {
        "id": "H_pXZNXeV25L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert out logits output to a class prediction\n",
        "predictions_logits = model.predict(test_dataset_batched)\n",
        "y_pred = np.argmax(predictions_logits, axis=1)"
      ],
      "metadata": {
        "id": "aUrAUpnTV25L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save predictions\n",
        "pred_df = pd.DataFrame({\"prediction\": y_pred})\n",
        "pred_df.to_csv(\"predictions_test_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "d19izWCyV25L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}